"""This module summarizes the functions used to generate the competencies.
It contains a multitude of functions that are used to extract competencies
from abstracts using different language models.

Returns:
    _type_: _description_
"""
import random
from transformers import (XLNetTokenizer, XLNetForQuestionAnsweringSimple,
                          BloomForCausalLM, BloomTokenizerFast,
                          pipeline, AutoTokenizer, OPTForCausalLM)
from torch import argmax
from torch.nn import functional as F
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import yaml

with open("database_api/db_creation/db_config.yml", "r",
          encoding='utf-8') as ymlfile:
    cfg = yaml.safe_load(ymlfile)


def get_competency_from_backend(abstract: str):
    """Returns a list of competencies using KeyBERT with optimized paramters.

    Args:
        abstract (str): A scientific abstract in text format

    Returns:
        list: list in the form of [(competency, score), ...]
    """
    kw_model = KeyBERT("distilbert-base-nli-mean-tokens")
    keywords = kw_model.extract_keywords(abstract,
                                         keyphrase_ngram_range=(1, 2),
                                         use_mmr=True, diversity=0.5)
    # filtered_keywords = [keywords for keyword in keywords if keyword[
    # 1] > 0.4]
    filtered_keywords = list(filter(lambda x: x[1] > 0.4, keywords))
    return filtered_keywords


def answer_abstract_question(abstract: str):
    """Answers the question "What is the most meaningful word in the text?"
    for a given abstract.

    Args:
        abstract (str): A scienfific abstract in text format

    Returns:
        json: The answer and the confidence of the model
    """
    context = abstract
    question = "What skill is mentioned in the text?"
    qa_model = pipeline("question-answering")
    answer = qa_model(question=question,
                      context=context, max_answer_length=5)

    return {"answer": answer["answer"], "confidence": answer["score"]}


def summarize(abstract: str):
    """Summarizes a given abstract

    Args:
        abstract (str): A scientific abstract in text format
    """
    summarizer = pipeline("summarization", model="google/pegasus-xsum")
    answer = summarizer(abstract, max_length=2000)[0]["summary_text"]
    return answer


def get_mock_competency():
    """Returns a mock competency

    Returns:
        str: A mock competency
    """
    competence_list = ["ner", "sentiment-analysis", "text-generation",
                       "text-classification", "question-answering",
                       "fill-mask", "summarization", "clonation",
                       "mathematics", "linear-algebra", "analysis"]

    return random.choice(competence_list)


def ask_galactica(abstract: str):
    """Returns galactica's answer to being asked what competencies an
    abstract author has.

    Args:
        abstract (str): A scientific abstract in text format

    Returns:
        list: A list of competencies generated by Galactica
    """
    tokenizer = AutoTokenizer.from_pretrained("facebook/galactica-1.3b")
    model = OPTForCausalLM.from_pretrained("facebook/galactica-1.3b")

    prompt = "The person who wrote this abstract is good at:"
    input_text = abstract + prompt
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids

    outputs = model.generate(input_ids)
    total_output = []
    for output in outputs:
        total_output.append(tokenizer.decode(output, skip_special_tokens=True))
    return total_output


def ask_xlnet(abstract: str):
    """Generates an answer to the question "What competency is mentioned in
    the abstract?" using XLNet.

    Args:
        abstract (str): A scientific abstract in text format

    Returns:
        list: A list of competencies generated by XLNet
    """
    tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased")
    model = XLNetForQuestionAnsweringSimple.from_pretrained("xlnet-base-cased",
                                                            return_dict=True)
    question = "What skill is mentioned in the abstract?"
    inputs = tokenizer.encode_plus(question, abstract,
                                   return_tensors='pt')
    output = model(**inputs)
    start_max = argmax(F.softmax(output.start_logits, dim=-1))
    end_max = argmax(F.softmax(output.end_logits, dim=-1)) + 1
    # Add one because of python list indexing
    answer = tokenizer.decode(inputs["input_ids"][0][start_max: end_max])
    return answer


def ask_bloom(abstract: str, method: int):
    """Generates an answer to the question "What competency is mentioned in
    the abstract?" using Bloom. A method can be chosen to generate the
    answer. The methods are: 0: Greedy Search, 1: Beam Search, 2: Sampling

    Args:
        abstract (str): An abstract in text format
        method (int): The method to use for generating the answer
    """
    model = BloomForCausalLM.from_pretrained("bigscience/bloom-1b7")
    tokenizer = BloomTokenizerFast.from_pretrained("bigscience/bloom-1b7")
    question = "What competency is mentioned in the abstract?"
    prompt = abstract + question
    result_length = 50
    inputs = tokenizer(prompt, return_tensors="pt")

    if method == 0:
        # Greedy Search
        tokenizer.decode(model.generate(inputs["input_ids"],
                                        max_length=result_length)[0])
    elif method == 1:
        # Beam Search
        print(tokenizer.decode(model.generate(inputs["input_ids"],
                                              max_length=result_length,
                                              num_beams=2,
                                              no_repeat_ngram_size=2,
                                              early_stopping=True)[0]))
    elif method == 2:
        # Sampling Top-k + Top-p
        print(tokenizer.decode(model.generate(inputs["input_ids"],
                                              max_length=result_length,
                                              do_sample=True,
                                              top_k=50,
                                              top_p=0.9)[0]))


def ask_keybert(abstract: str):
    """Extracts keywords from an abstract using KeyBert.

    Args:
        abstract (str): A scientific abstract in text format
    """
    kw_model = KeyBERT()
    keywords = kw_model.extract_keywords(abstract,
                                         keyphrase_ngram_range=(1, 3))
    return keywords


def get_category_of_competence(competence: str):
    """Maps one of the 33 categories to a given competence based on
    the competence's similarity to the category's keywords.

    Args:
        competence (str): The string representation of a competence
    """
    categories = cfg["categories"]

    competency_and_categories = [competence] + categories
    model = SentenceTransformer("bert-base-nli-mean-tokens")
    categories_embeddings = model.encode(competency_and_categories)
    similarities = cosine_similarity([categories_embeddings[0]],
                                     categories_embeddings[1:])
    # print(similarities)
    # Get indices of similarities where similarity greater than 0.6
    # indices = [i for i, x in enumerate(similarities[0]) if x > 0.7]

    # Get index with maximum similarity
    index = similarities[0].argmax()
    return index


def generate_text(abstract: str):
    """Generates text based on an abstract using GPT-Neo.

    Args:
        abstract (str): A scientific abstract in text format

    Returns:
        list: A list of generated texts
    """
    prompt = "The competencies of the person who wrote this abstract are:"
    generator = pipeline(
                         "text-generation",
                         model="EleutherAI/gpt-neo-2.7B",
                         min_length=100,
                         max_length=100
                         )
    return generator(abstract + prompt)
